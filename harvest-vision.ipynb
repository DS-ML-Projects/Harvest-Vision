{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9342194,"sourceType":"datasetVersion","datasetId":5661695}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install graphviz pydotplus six imblearn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T21:58:25.409800Z","iopub.execute_input":"2024-12-23T21:58:25.410159Z","iopub.status.idle":"2024-12-23T21:58:29.374375Z","shell.execute_reply.started":"2024-12-23T21:58:25.410130Z","shell.execute_reply":"2024-12-23T21:58:29.373187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier, StackingClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom imblearn.over_sampling import SMOTE\nimport warnings\nimport os\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T21:58:29.435350Z","iopub.execute_input":"2024-12-23T21:58:29.435731Z","iopub.status.idle":"2024-12-23T21:58:29.448917Z","shell.execute_reply.started":"2024-12-23T21:58:29.435694Z","shell.execute_reply":"2024-12-23T21:58:29.448082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/agriculture-crop-yield/crop_yield.csv')\ndf_sample = df.sample(n=100000, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T21:58:29.449936Z","iopub.execute_input":"2024-12-23T21:58:29.450299Z","iopub.status.idle":"2024-12-23T21:58:32.242254Z","shell.execute_reply.started":"2024-12-23T21:58:29.450268Z","shell.execute_reply":"2024-12-23T21:58:32.241162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numerical_features = ['Rainfall_mm', 'Temperature_Celsius', 'Days_to_Harvest', 'Yield_tons_per_hectare']\n\n# Plot distributions (histograms with KDE)\nplt.figure(figsize=(15, 10))\nfor i, feature in enumerate(numerical_features, 1):\n    plt.subplot(2, 2, i)\n    sns.histplot(df_sample[feature], kde=True, bins=30)\n    plt.title(f'Distribution of {feature}')\n    plt.xlabel(feature)\n    plt.ylabel('Count')\nplt.tight_layout()\nplt.savefig('numerical_distributions.png')\nplt.show()\n\n# Plot box plots to identify outliers\nplt.figure(figsize=(15, 10))\nfor i, feature in enumerate(numerical_features, 1):\n    plt.subplot(2, 2, i)\n    sns.boxplot(y=df_sample[feature])\n    plt.title(f'Box Plot of {feature}')\nplt.tight_layout()\nplt.savefig('numerical_boxplots.png')\nplt.show()\n\n# Handle outliers using IQR method\ndef remove_outliers(df, column):\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n    return df\n\n# Apply outlier removal for numerical features\ndf_sample_cleaned = df_sample.copy()\nfor feature in numerical_features:\n    df_sample_cleaned = remove_outliers(df_sample_cleaned, feature)\n\n# Display the number of rows before and after outlier removal\nprint(f\"Original dataset size: {len(df_sample)}\")\nprint(f\"Dataset size after outlier removal: {len(df_sample_cleaned)}\")\n\n# Update df_sample to the cleaned version\ndf_sample = df_sample_cleaned","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocessing: Encode categorical variables and convert yield to classification\nle_region = LabelEncoder()\nle_soil = LabelEncoder()\nle_crop = LabelEncoder()\nle_weather = LabelEncoder()\n\ndf_sample['Region'] = le_region.fit_transform(df_sample['Region'])\ndf_sample['Soil_Type'] = le_soil.fit_transform(df_sample['Soil_Type'])\ndf_sample['Crop'] = le_crop.fit_transform(df_sample['Crop'])\ndf_sample['Weather_Condition'] = le_weather.fit_transform(df_sample['Weather_Condition'])\ndf_sample['Fertilizer_Used'] = df_sample['Fertilizer_Used'].astype(int)\ndf_sample['Irrigation_Used'] = df_sample['Irrigation_Used'].astype(int)\n\n# Define features and target (convert Yield to classification: High/Low based on median)\nmedian_yield = df_sample['Yield_tons_per_hectare'].median()\ndf_sample['Yield_Class'] = (df_sample['Yield_tons_per_hectare'] > median_yield).astype(int)\n\nX = df_sample[['Region', 'Soil_Type', 'Crop', 'Rainfall_mm', 'Temperature_Celsius', \n               'Fertilizer_Used', 'Irrigation_Used', 'Weather_Condition', 'Days_to_Harvest']]\ny = df_sample['Yield_Class']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split and scale the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Handle class imbalance with SMOTE\nsmote = SMOTE(random_state=42)\nX_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize correlation matrix for numerical features\nnumeric_df = df_sample.select_dtypes(include=['int64', 'float64'])\ncorrelation_matrix = numeric_df.corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\nplt.title('Correlation Heatmap')\nplt.savefig('correlation_heatmap.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train and evaluate Decision Tree with readable visualization\ndt = DecisionTreeClassifier(max_depth=3, random_state=42)  # Limit depth for readability\ndt.fit(X_train_balanced, y_train_balanced)\ny_pred_dt = dt.predict(X_test_scaled)\nprint(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\nprint(classification_report(y_test, y_pred_dt, zero_division=0))\n\n# Visualize Decision Tree\nplt.figure(figsize=(20,10))\nplot_tree(dt, feature_names=X.columns, class_names=['Low', 'High'], filled=True, rounded=True, impurity=False, fontsize=10)\nplt.title(\"Decision Tree for Yield Classification\")\nplt.savefig('decision_tree.png')\nplt.show()\n\n# Confusion Matrix for Decision Tree\ncm_dt = confusion_matrix(y_test, y_pred_dt)\ndisp_dt = ConfusionMatrixDisplay(confusion_matrix=cm_dt, display_labels=['Low', 'High'])\ndisp_dt.plot(cmap='Blues')\nplt.title('Confusion Matrix - Decision Tree')\nplt.savefig('confusion_matrix_dt.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train and evaluate Logistic Regression\nlr = LogisticRegression(random_state=42, max_iter=1000)\nlr.fit(X_train_balanced, y_train_balanced)\ny_pred_lr = lr.predict(X_test_scaled)\nprint(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\nprint(classification_report(y_test, y_pred_lr, zero_division=0))\n\n# Confusion Matrix for Logistic Regression\ncm_lr = confusion_matrix(y_test, y_pred_lr)\ndisp_lr = ConfusionMatrixDisplay(confusion_matrix=cm_lr, display_labels=['Low', 'High'])\ndisp_lr.plot(cmap='Blues')\nplt.title('Confusion Matrix - Logistic Regression')\nplt.savefig('confusion_matrix_lr.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train and evaluate Linear SVM\nsvm = LinearSVC(random_state=42, max_iter=1000)\nsvm.fit(X_train_balanced, y_train_balanced)\ny_pred_svm = svm.predict(X_test_scaled)\nprint(\"Linear SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\nprint(classification_report(y_test, y_pred_svm, zero_division=0))\n\n# Confusion Matrix for Linear SVM\ncm_svm = confusion_matrix(y_test, y_pred_svm)\ndisp_svm = ConfusionMatrixDisplay(confusion_matrix=cm_svm, display_labels=['Low', 'High'])\ndisp_svm.plot(cmap='Blues')\nplt.title('Confusion Matrix - Linear SVM')\nplt.savefig('confusion_matrix_svm.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train and evaluate optimized KNN (with smaller sample for speed)\nX_train_small, _, y_train_small, _ = train_test_split(X_train_balanced, y_train_balanced, train_size=0.1, random_state=42)\nknn = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree', n_jobs=-1)\nknn.fit(X_train_small, y_train_small)\ny_pred_knn = knn.predict(X_test_scaled)\nprint(\"KNN Accuracy (Small Sample):\", accuracy_score(y_test, y_pred_knn))\nprint(classification_report(y_test, y_pred_knn, zero_division=0))\n\n# Hyperparameter tuning for KNN (on small sample)\nparam_grid_knn = {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}\ngrid_search_knn = GridSearchCV(KNeighborsClassifier(algorithm='ball_tree', n_jobs=-1), \n                              param_grid_knn, cv=3, scoring='f1_weighted', n_jobs=-1)\ngrid_search_knn.fit(X_train_small, y_train_small)\nbest_knn = grid_search_knn.best_estimator_\nprint(\"Best KNN Params:\", grid_search_knn.best_params_)\n\n# Evaluate best KNN\ny_pred_best_knn = best_knn.predict(X_test_scaled)\nprint(\"Best KNN Accuracy:\", accuracy_score(y_test, y_pred_best_knn))\nprint(classification_report(y_test, y_pred_best_knn, zero_division=0))\n\n# Confusion Matrix for Best KNN\ncm_knn = confusion_matrix(y_test, y_pred_best_knn)\ndisp_knn = ConfusionMatrixDisplay(confusion_matrix=cm_knn, display_labels=['Low', 'High'])\ndisp_knn.plot(cmap='Blues')\nplt.title('Confusion Matrix - Best KNN')\nplt.savefig('confusion_matrix_knn.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train and evaluate Random Forest\nrf = RandomForestClassifier(random_state=42, n_estimators=100, n_jobs=-1)\nrf.fit(X_train_balanced, y_train_balanced)\ny_pred_rf = rf.predict(X_test_scaled)\nprint(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\nprint(classification_report(y_test, y_pred_rf, zero_division=0))\n\n# Confusion Matrix for Random Forest\ncm_rf = confusion_matrix(y_test, y_pred_rf)\ndisp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=['Low', 'High'])\ndisp_rf.plot(cmap='Blues')\nplt.title('Confusion Matrix - Random Forest')\nplt.savefig('confusion_matrix_rf.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train and evaluate Ensemble (Stacking)\nestimators = [('rf', rf), ('svm', svm)]\nstacking = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\nstacking.fit(X_train_balanced, y_train_balanced)\ny_pred_stacking = stacking.predict(X_test_scaled)\nprint(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, y_pred_stacking))\nprint(classification_report(y_pred_stacking, y_test, zero_division=0))\n\n# Confusion Matrix for Stacking Classifier\ncm_stacking = confusion_matrix(y_test, y_pred_stacking)\ndisp_stacking = ConfusionMatrixDisplay(confusion_matrix=cm_stacking, display_labels=['Low', 'High'])\ndisp_stacking.plot(cmap='Blues')\nplt.title('Confusion Matrix - Stacking Classifier')\nplt.savefig('confusion_matrix_stacking.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to plot learning curves\ndef plot_learning_curve(estimator, title, X, y, cv=3, train_sizes=np.linspace(0.1, 1.0, 5)):\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=-1, train_sizes=train_sizes, scoring='accuracy'\n    )\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    plt.figure(figsize=(10, 6))\n    plt.title(title)\n    plt.xlabel(\"Training Examples\")\n    plt.ylabel(\"Accuracy\")\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training Accuracy\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-Validation Accuracy\")\n\n    plt.legend(loc=\"best\")\n    os.makedirs('plots', exist_ok=True)\n    plt.savefig(f'plots/learning_curve_{title.lower().replace(\" \", \"_\")}.png')\n    plt.show()\n\n# Plot learning curves for all models\nmodels = {\n    'Decision Tree': dt,\n    'Logistic Regression': lr,\n    'Linear SVM': svm,\n    'KNN': best_knn,\n    'Random Forest': rf,\n    'Stacking Classifier': stacking\n}\n\nfor name, model in models.items():\n    print(f\"Generating learning curve for {name}...\")\n    plot_learning_curve(model, f\"Learning Curve - {name}\", X_train_balanced, y_train_balanced)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Store best models for comparison\nbest_models = {\n    'Decision Tree': dt,\n    'Logistic Regression': lr,\n    'Linear SVM': svm,\n    'KNN': best_knn,\n    'Random Forest': rf,\n    'Stacking Classifier': stacking\n}\n\n# Compare all models and find the best\nmodel_accuracies = {}\nfor name, model in best_models.items():\n    y_pred = model.predict(X_test_scaled)\n    accuracy = accuracy_score(y_test, y_pred)\n    model_accuracies[name] = accuracy\n    print(f\"{name} Accuracy: {accuracy:.6f}\")\n\n# Identify the best model\nbest_model_name = max(model_accuracies, key=model_accuracies.get)\nbest_accuracy = model_accuracies[best_model_name]\nprint(f\"\\nBest Model: {best_model_name} with Accuracy: {best_accuracy:.6f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}